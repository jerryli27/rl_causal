{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mine_exp",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYRDoGgvWqHp",
        "colab_type": "code",
        "outputId": "407adfaf-93e4-4127-b123-2647b9cd51fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
            "\u001b[K     |████████████████████████████████| 87.9MB 44.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_JSAqxCWzWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWQJxWu-ohrU",
        "colab_type": "text"
      },
      "source": [
        "# General setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fflGzvEDoiss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# def get_t_network_general(x, y, hidden=100):\n",
        "#   x_shape = keras.backend.int_shape(x)\n",
        "#   y_shape = keras.backend.int_shape(y)\n",
        "  \n",
        "#   x = keras.layers.Flatten()(x)\n",
        "#   y = keras.layers.Flatten()(y)\n",
        "  \n",
        "#   x = keras.layers.Dense(hidden)(x)\n",
        "#   y = keras.layers.Dense(hidden)(y)\n",
        "  \n",
        "#   t = keras.layers.concatenate([x, y])\n",
        "#   t = keras.layers.Dense(hidden)(t)\n",
        "#   t = keras.layers.Dense(1)(t)\n",
        "#   return t\n",
        "\n",
        "\n",
        "\n",
        "def get_t_network_general(x, y_actual, y_random, hidden=100):\n",
        "  \n",
        "  \n",
        "  model_x = keras.models.Sequential([\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(hidden),\n",
        "      keras.layers.LeakyReLU(),\n",
        "  ])\n",
        "  \n",
        "  model_y = keras.models.Sequential([\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(hidden),\n",
        "      keras.layers.LeakyReLU(),\n",
        "  ])\n",
        "  \n",
        "  x_hidden = model_x(x)\n",
        "  y_actual_hidden = model_y(y_actual)\n",
        "  y_random_hidden = model_y(y_random)\n",
        "  \n",
        "  t_actual = keras.layers.concatenate([x_hidden, y_actual_hidden])\n",
        "  t_random = keras.layers.concatenate([x_hidden, y_random_hidden])\n",
        "  \n",
        "  model_t = keras.models.Sequential([\n",
        "      keras.layers.Dense(hidden),\n",
        "      keras.layers.LeakyReLU(),\n",
        "      keras.layers.Dense(1)\n",
        "  ])\n",
        "  \n",
        "  t_actual = model_t(t_actual)\n",
        "  t_random = model_t(t_random)\n",
        "  return t_actual, t_random\n",
        "\n",
        "def get_v_network(t_actual, t_random):\n",
        "  v = keras.backend.mean(t_actual) - keras.backend.log(keras.backend.mean(keras.backend.exp(t_random)))\n",
        "  return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeHvul5O2XAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_general_data(data_size, dim, correlation=0.0):\n",
        "  var = 0.2\n",
        "  mean = np.zeros(dim + dim)\n",
        "  \n",
        "  cov = np.eye(dim * 2)\n",
        "  \n",
        "  cov[dim:, :dim] += np.flip(np.eye(dim), axis=0) * correlation\n",
        "  cov[:dim, dim:] += np.flip(np.eye(dim), axis=0) * correlation\n",
        "\n",
        "  x_y = np.expand_dims(np.random.multivariate_normal(mean, cov, data_size), axis=-1)\n",
        "  x = x_y[:, :dim]\n",
        "  actual_y = x_y[:, dim:]\n",
        "  \n",
        "  random_x_y = np.expand_dims(np.random.multivariate_normal(mean, cov, data_size), axis=-1)\n",
        "  random_y = random_x_y[:, dim:]\n",
        "  \n",
        "\n",
        "  return {\n",
        "      'x_input': x,\n",
        "      'actual_y_input': actual_y,\n",
        "      'random_y_input': random_y,\n",
        "  }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35HJxwacpXDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd2dc9e0-10d1-4ef6-e850-e1f2b0346546"
      },
      "source": [
        "correlations = np.linspace(-0.9,0.9,15)\n",
        "corr_score = []\n",
        "\n",
        "\n",
        "for correlation in correlations:\n",
        "\n",
        "  x_shape = (2, 1)\n",
        "  y_shape = (2, 1)\n",
        "\n",
        "  train_data_size = 10000\n",
        "  test_data_size = train_data_size // 10\n",
        "\n",
        "  x_train = get_general_data(train_data_size, dim=x_shape[0], correlation=correlation)\n",
        "  y_train = np.zeros(train_data_size)\n",
        "  x_test = get_general_data(test_data_size, dim=x_shape[0], correlation=correlation)\n",
        "  y_test = np.zeros(test_data_size)\n",
        "  x_tiny = get_general_data(1, dim=x_shape[0], correlation=correlation)\n",
        "  y_tiny = np.zeros(1)\n",
        "\n",
        "  x_input = keras.layers.Input(shape=x_shape, dtype='float', name='x_input')\n",
        "  actual_y_input = keras.layers.Input(shape=y_shape, dtype='float', name='actual_y_input')\n",
        "  random_y_input = keras.layers.Input(shape=y_shape, dtype='float', name='random_y_input')\n",
        "\n",
        "  # t_actual = get_t_network_general(x_input, actual_y_input)\n",
        "  # t_random = get_t_network_general(x_input, random_y_input)\n",
        "  t_actual, t_random = get_t_network_general(x_input, actual_y_input, random_y_input)\n",
        "\n",
        "  v = get_v_network(t_actual, t_random)\n",
        "\n",
        "  # TODO: the gradient of v is biased. I am not sure how to use the moving average\n",
        "  # yet exactly.\n",
        "\n",
        "  model = keras.models.Model(inputs=[x_input, actual_y_input, random_y_input], outputs=[v])\n",
        "  # optimizer = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "  optimizer = keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None)\n",
        "\n",
        "\n",
        "  def neg_identity_loss(y_true, y_pred):\n",
        "    # Maximize.\n",
        "    return -y_pred\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=neg_identity_loss,\n",
        "  #   metrics=[tf.keras.metrics.Mean()]\n",
        "  )\n",
        "\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "  score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "  print('test score: ', score)\n",
        "  \n",
        "  corr_score.append(score)\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.   0.   0.  -0.9]\n",
            " [ 0.   1.  -0.9  0. ]\n",
            " [ 0.  -0.9  1.   0. ]\n",
            " [-0.9  0.   0.   1. ]]\n",
            "[[ 1.   0.   0.  -0.9]\n",
            " [ 0.   1.  -0.9  0. ]\n",
            " [ 0.  -0.9  1.   0. ]\n",
            " [-0.9  0.   0.   1. ]]\n",
            "[[ 1.   0.   0.  -0.9]\n",
            " [ 0.   1.  -0.9  0. ]\n",
            " [ 0.  -0.9  1.   0. ]\n",
            " [-0.9  0.   0.   1. ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: -1.3285\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -1.5660\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -1.5696\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -1.6506\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -1.6808\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -1.6755\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -1.6955\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -1.6664\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -1.7053\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -1.6952\n",
            "1000/1000 [==============================] - 0s 86us/sample - loss: -1.6446\n",
            "test score:  -1.644617193222046\n",
            "[[ 1.          0.          0.         -0.77142857]\n",
            " [ 0.          1.         -0.77142857  0.        ]\n",
            " [ 0.         -0.77142857  1.          0.        ]\n",
            " [-0.77142857  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.77142857]\n",
            " [ 0.          1.         -0.77142857  0.        ]\n",
            " [ 0.         -0.77142857  1.          0.        ]\n",
            " [-0.77142857  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.77142857]\n",
            " [ 0.          1.         -0.77142857  0.        ]\n",
            " [ 0.         -0.77142857  1.          0.        ]\n",
            " [-0.77142857  0.          0.          1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 93us/sample - loss: -0.6975\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -0.7844\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.7964\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.8370\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 93us/sample - loss: -0.8513\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 89us/sample - loss: -0.8497\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 86us/sample - loss: -0.8527\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: -0.8652\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: -0.8465\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: -0.8618\n",
            "1000/1000 [==============================] - 0s 89us/sample - loss: -0.8756\n",
            "test score:  -0.875563108921051\n",
            "[[ 1.          0.          0.         -0.64285714]\n",
            " [ 0.          1.         -0.64285714  0.        ]\n",
            " [ 0.         -0.64285714  1.          0.        ]\n",
            " [-0.64285714  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.64285714]\n",
            " [ 0.          1.         -0.64285714  0.        ]\n",
            " [ 0.         -0.64285714  1.          0.        ]\n",
            " [-0.64285714  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.64285714]\n",
            " [ 0.          1.         -0.64285714  0.        ]\n",
            " [ 0.         -0.64285714  1.          0.        ]\n",
            " [-0.64285714  0.          0.          1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 94us/sample - loss: -0.4307\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.4869\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.5008\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 75us/sample - loss: -0.4990\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 75us/sample - loss: -0.5146\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.5159\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.5186\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.5234\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.5204\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.5231\n",
            "1000/1000 [==============================] - 0s 85us/sample - loss: -0.4647\n",
            "test score:  -0.46469262027740477\n",
            "[[ 1.          0.          0.         -0.51428571]\n",
            " [ 0.          1.         -0.51428571  0.        ]\n",
            " [ 0.         -0.51428571  1.          0.        ]\n",
            " [-0.51428571  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.51428571]\n",
            " [ 0.          1.         -0.51428571  0.        ]\n",
            " [ 0.         -0.51428571  1.          0.        ]\n",
            " [-0.51428571  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.51428571]\n",
            " [ 0.          1.         -0.51428571  0.        ]\n",
            " [ 0.         -0.51428571  1.          0.        ]\n",
            " [-0.51428571  0.          0.          1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: -0.2406\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.2795\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: -0.2803\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.2837\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.2900\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: -0.3001\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.2912\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.2991\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.2964\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -0.3012\n",
            "1000/1000 [==============================] - 0s 88us/sample - loss: -0.3023\n",
            "test score:  -0.3022585391998291\n",
            "[[ 1.          0.          0.         -0.38571429]\n",
            " [ 0.          1.         -0.38571429  0.        ]\n",
            " [ 0.         -0.38571429  1.          0.        ]\n",
            " [-0.38571429  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.38571429]\n",
            " [ 0.          1.         -0.38571429  0.        ]\n",
            " [ 0.         -0.38571429  1.          0.        ]\n",
            " [-0.38571429  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.38571429]\n",
            " [ 0.          1.         -0.38571429  0.        ]\n",
            " [ 0.         -0.38571429  1.          0.        ]\n",
            " [-0.38571429  0.          0.          1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 97us/sample - loss: -0.1236\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.1458\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 75us/sample - loss: -0.1445\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.1544\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: -0.1561\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.1531\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.1575\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.1586\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.1582\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.1607\n",
            "1000/1000 [==============================] - 0s 85us/sample - loss: -0.1569\n",
            "test score:  -0.1569217581152916\n",
            "[[ 1.          0.          0.         -0.25714286]\n",
            " [ 0.          1.         -0.25714286  0.        ]\n",
            " [ 0.         -0.25714286  1.          0.        ]\n",
            " [-0.25714286  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.25714286]\n",
            " [ 0.          1.         -0.25714286  0.        ]\n",
            " [ 0.         -0.25714286  1.          0.        ]\n",
            " [-0.25714286  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.25714286]\n",
            " [ 0.          1.         -0.25714286  0.        ]\n",
            " [ 0.         -0.25714286  1.          0.        ]\n",
            " [-0.25714286  0.          0.          1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 96us/sample - loss: -0.0446\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0540\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.0604\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.0625\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0649\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0627\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: -0.0648\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0659\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0665\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.0679\n",
            "1000/1000 [==============================] - 0s 93us/sample - loss: -0.0657\n",
            "test score:  -0.06568033385276795\n",
            "[[ 1.          0.          0.         -0.12857143]\n",
            " [ 0.          1.         -0.12857143  0.        ]\n",
            " [ 0.         -0.12857143  1.          0.        ]\n",
            " [-0.12857143  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.12857143]\n",
            " [ 0.          1.         -0.12857143  0.        ]\n",
            " [ 0.         -0.12857143  1.          0.        ]\n",
            " [-0.12857143  0.          0.          1.        ]]\n",
            "[[ 1.          0.          0.         -0.12857143]\n",
            " [ 0.          1.         -0.12857143  0.        ]\n",
            " [ 0.         -0.12857143  1.          0.        ]\n",
            " [-0.12857143  0.          0.          1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 100us/sample - loss: 4.8171e-04\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.0054\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0065\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 86us/sample - loss: -0.0074\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0077\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0086\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0093\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.0096\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: -0.0088\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0094\n",
            "1000/1000 [==============================] - 0s 83us/sample - loss: -0.0055\n",
            "test score:  -0.0054957613945007325\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 1.11022302e-16]\n",
            " [0.00000000e+00 1.00000000e+00 1.11022302e-16 0.00000000e+00]\n",
            " [0.00000000e+00 1.11022302e-16 1.00000000e+00 0.00000000e+00]\n",
            " [1.11022302e-16 0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 1.11022302e-16]\n",
            " [0.00000000e+00 1.00000000e+00 1.11022302e-16 0.00000000e+00]\n",
            " [0.00000000e+00 1.11022302e-16 1.00000000e+00 0.00000000e+00]\n",
            " [1.11022302e-16 0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
            "[[1.00000000e+00 0.00000000e+00 0.00000000e+00 1.11022302e-16]\n",
            " [0.00000000e+00 1.00000000e+00 1.11022302e-16 0.00000000e+00]\n",
            " [0.00000000e+00 1.11022302e-16 1.00000000e+00 0.00000000e+00]\n",
            " [1.11022302e-16 0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 99us/sample - loss: 0.0099\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: 0.0031\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: 0.0011\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: 7.7117e-04\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: 4.0331e-04\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -7.7347e-06\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: 2.7756e-04\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: 8.6245e-05\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: 6.4034e-05\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: 9.0535e-06\n",
            "1000/1000 [==============================] - 0s 92us/sample - loss: 5.6303e-04\n",
            "test score:  0.0005630296021699906\n",
            "[[1.         0.         0.         0.12857143]\n",
            " [0.         1.         0.12857143 0.        ]\n",
            " [0.         0.12857143 1.         0.        ]\n",
            " [0.12857143 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.12857143]\n",
            " [0.         1.         0.12857143 0.        ]\n",
            " [0.         0.12857143 1.         0.        ]\n",
            " [0.12857143 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.12857143]\n",
            " [0.         1.         0.12857143 0.        ]\n",
            " [0.         0.12857143 1.         0.        ]\n",
            " [0.12857143 0.         0.         1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: -0.0060\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.0119\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -0.0138\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.0140\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0163\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.0161\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -0.0155\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0158\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.0178\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.0179\n",
            "1000/1000 [==============================] - 0s 85us/sample - loss: -0.0115\n",
            "test score:  -0.011467337012290954\n",
            "[[1.         0.         0.         0.25714286]\n",
            " [0.         1.         0.25714286 0.        ]\n",
            " [0.         0.25714286 1.         0.        ]\n",
            " [0.25714286 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.25714286]\n",
            " [0.         1.         0.25714286 0.        ]\n",
            " [0.         0.25714286 1.         0.        ]\n",
            " [0.25714286 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.25714286]\n",
            " [0.         1.         0.25714286 0.        ]\n",
            " [0.         0.25714286 1.         0.        ]\n",
            " [0.25714286 0.         0.         1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 94us/sample - loss: -0.0491\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.0600\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -0.0647\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.0664\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: -0.0682\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: -0.0692\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 87us/sample - loss: -0.0705\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 86us/sample - loss: -0.0708\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 87us/sample - loss: -0.0709\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 89us/sample - loss: -0.0740\n",
            "1000/1000 [==============================] - 0s 88us/sample - loss: -0.1021\n",
            "test score:  -0.10205503438413144\n",
            "[[1.         0.         0.         0.38571429]\n",
            " [0.         1.         0.38571429 0.        ]\n",
            " [0.         0.38571429 1.         0.        ]\n",
            " [0.38571429 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.38571429]\n",
            " [0.         1.         0.38571429 0.        ]\n",
            " [0.         0.38571429 1.         0.        ]\n",
            " [0.38571429 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.38571429]\n",
            " [0.         1.         0.38571429 0.        ]\n",
            " [0.         0.38571429 1.         0.        ]\n",
            " [0.38571429 0.         0.         1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 106us/sample - loss: -0.1258\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.1396\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.1479\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: -0.1526\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: -0.1536\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: -0.1551\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.1557\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.1569\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: -0.1620\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.1607\n",
            "1000/1000 [==============================] - 0s 86us/sample - loss: -0.2495\n",
            "test score:  -0.24950868797302245\n",
            "[[1.         0.         0.         0.51428571]\n",
            " [0.         1.         0.51428571 0.        ]\n",
            " [0.         0.51428571 1.         0.        ]\n",
            " [0.51428571 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.51428571]\n",
            " [0.         1.         0.51428571 0.        ]\n",
            " [0.         0.51428571 1.         0.        ]\n",
            " [0.51428571 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.51428571]\n",
            " [0.         1.         0.51428571 0.        ]\n",
            " [0.         0.51428571 1.         0.        ]\n",
            " [0.51428571 0.         0.         1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 101us/sample - loss: -0.2273\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.2686\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.2773\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.2792\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: -0.2897\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.2846\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.2881\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.2875\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.3038\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: -0.2993\n",
            "1000/1000 [==============================] - 0s 95us/sample - loss: -0.3264\n",
            "test score:  -0.32640633010864256\n",
            "[[1.         0.         0.         0.64285714]\n",
            " [0.         1.         0.64285714 0.        ]\n",
            " [0.         0.64285714 1.         0.        ]\n",
            " [0.64285714 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.64285714]\n",
            " [0.         1.         0.64285714 0.        ]\n",
            " [0.         0.64285714 1.         0.        ]\n",
            " [0.64285714 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.64285714]\n",
            " [0.         1.         0.64285714 0.        ]\n",
            " [0.         0.64285714 1.         0.        ]\n",
            " [0.64285714 0.         0.         1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: -0.4285\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.4887\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.4940\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 75us/sample - loss: -0.5115\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 74us/sample - loss: -0.5081\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -0.5145\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.5198\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -0.5208\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -0.5144\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.5214\n",
            "1000/1000 [==============================] - 0s 89us/sample - loss: -0.4604\n",
            "test score:  -0.46040672647953035\n",
            "[[1.         0.         0.         0.77142857]\n",
            " [0.         1.         0.77142857 0.        ]\n",
            " [0.         0.77142857 1.         0.        ]\n",
            " [0.77142857 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.77142857]\n",
            " [0.         1.         0.77142857 0.        ]\n",
            " [0.         0.77142857 1.         0.        ]\n",
            " [0.77142857 0.         0.         1.        ]]\n",
            "[[1.         0.         0.         0.77142857]\n",
            " [0.         1.         0.77142857 0.        ]\n",
            " [0.         0.77142857 1.         0.        ]\n",
            " [0.77142857 0.         0.         1.        ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 102us/sample - loss: -0.7338\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -0.8629\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.8845\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.9131\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 78us/sample - loss: -0.8957\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.9128\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.9389\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.9230\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -0.9223\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 82us/sample - loss: -0.9342\n",
            "1000/1000 [==============================] - 0s 89us/sample - loss: -0.8932\n",
            "test score:  -0.8932220335006714\n",
            "[[1.  0.  0.  0.9]\n",
            " [0.  1.  0.9 0. ]\n",
            " [0.  0.9 1.  0. ]\n",
            " [0.9 0.  0.  1. ]]\n",
            "[[1.  0.  0.  0.9]\n",
            " [0.  1.  0.9 0. ]\n",
            " [0.  0.9 1.  0. ]\n",
            " [0.9 0.  0.  1. ]]\n",
            "[[1.  0.  0.  0.9]\n",
            " [0.  1.  0.9 0. ]\n",
            " [0.  0.9 1.  0. ]\n",
            " [0.9 0.  0.  1. ]]\n",
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 98us/sample - loss: -1.2529\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -1.5266\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -1.5258\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -1.5448\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -1.5581\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 81us/sample - loss: -1.5810\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 76us/sample - loss: -1.5895\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 75us/sample - loss: -1.6101\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -1.6008\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 75us/sample - loss: -1.6188\n",
            "1000/1000 [==============================] - 0s 80us/sample - loss: -1.5511\n",
            "test score:  -1.551110692024231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe12cf81748>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESkDgWDiIjUoWAQEZE6/j+WBWyStmloQQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isYFf5-BEdVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  \n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(correlations, corr_score, color='tab:blue')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egw08XMdvjl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# t_predict = keras.backend.function(\n",
        "#     inputs=[x_input, actual_y_input, random_y_input], outputs=[t_actual, t_random])\n",
        "\n",
        "# t_predict_output = t_predict(\n",
        "#     [x_test['x_input'], x_test['actual_y_input'], \n",
        "#      x_test['random_y_input'],])\n",
        "\n",
        "\n",
        "# v_predict = keras.backend.function(\n",
        "#     inputs=[x_input, actual_y_input, random_y_input], outputs=[v])\n",
        "# v_predict_output = v_predict(\n",
        "#     [x_tiny['x_input'], x_tiny['actual_y_input'], \n",
        "#      x_tiny['random_y_input'],])\n",
        "\n",
        "\n",
        "# v_predict_output_test = v_predict(\n",
        "#     [x_test['x_input'], x_test['actual_y_input'], \n",
        "#      x_test['random_y_input'],])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwPlWIyarVnp",
        "colab_type": "text"
      },
      "source": [
        "# Exp 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-VvUexhW1Yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Env(object):\n",
        "  def __init__(self):\n",
        "    self.state_space = (3, 1)\n",
        "    self.action_space = (2, 1)\n",
        "    \n",
        "  def get_data(self, n, correlation=0.0):\n",
        "    \n",
        "    ret = {\n",
        "        'state': [],\n",
        "        'actual_action': [],\n",
        "        'next_state': [],\n",
        "    }\n",
        "    for i in range(n):\n",
        "      state = np.random.rand(*self.state_space)\n",
        "      action = np.random.rand(*self.action_space)\n",
        "      \n",
        "      next_state = np.random.rand(*self.state_space)\n",
        "      \n",
        "#       if action[0,0] > 0.5 and action[1,0] > 0.5:\n",
        "#         next_state = np.random.rand(*self.state_space)\n",
        "#       else:\n",
        "#         next_state = np.random.rand(*self.state_space) / 2 + 0.5\n",
        "      \n",
        "      ret['state'].append(state)\n",
        "      ret['actual_action'].append(action)\n",
        "      ret['next_state'].append(next_state)\n",
        "      \n",
        "    ret['state'] = np.array(ret['state'])\n",
        "    ret['actual_action'] = np.array(ret['actual_action'])\n",
        "    ret['next_state'] = np.array(ret['next_state'])\n",
        "    ret['random_action'] = self.get_random_action(n)\n",
        "    \n",
        "    return ret\n",
        "  \n",
        "  def get_random_action(self, n):\n",
        "    action = np.random.rand(*((n,) + self.action_space))\n",
        "    return action\n",
        "\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PndGNl5blcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predicted_next_state(state, action):\n",
        "  state_shape = keras.backend.int_shape(state)\n",
        "  action_shape = keras.backend.int_shape(action)\n",
        "  \n",
        "  state = keras.backend.expand_dims(state, axis=-2)\n",
        "  action = keras.backend.expand_dims(action, axis=-3)\n",
        "  state = tf.keras.backend.repeat_elements(state, rep=action_shape[1], axis=-2)\n",
        "  action = tf.keras.backend.repeat_elements(action, rep=state_shape[1], axis=-3)\n",
        "  state_action = keras.layers.concatenate([state, action])\n",
        "  next_state = keras.layers.Dense(state_shape[-1])(state_action)\n",
        "  \n",
        "  next_state = tf.keras.backend.sum(next_state, axis=-2)\n",
        "  \n",
        "  return next_state\n",
        "\n",
        "\n",
        "def get_t_network(state, actual_action, next_state, random_action, hidden=100):\n",
        "  \n",
        "  \n",
        "  model_x = keras.models.Sequential([\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(hidden),\n",
        "      keras.layers.LeakyReLU(),\n",
        "  ])\n",
        "  \n",
        "  model_y = keras.models.Sequential([\n",
        "      keras.layers.Flatten(),\n",
        "      keras.layers.Dense(hidden),\n",
        "      keras.layers.LeakyReLU(),\n",
        "  ])\n",
        "  \n",
        "  x = keras.layers.concatenate([state, next_state])\n",
        "  \n",
        "  \n",
        "  x_hidden = model_x(x)\n",
        "  y_actual_hidden = model_y(actual_action)\n",
        "  y_random_hidden = model_y(random_action)\n",
        "  \n",
        "  t_actual = keras.layers.concatenate([x_hidden, y_actual_hidden])\n",
        "  t_random = keras.layers.concatenate([x_hidden, y_random_hidden])\n",
        "  \n",
        "  model_t = keras.models.Sequential([\n",
        "      keras.layers.Dense(hidden),\n",
        "      keras.layers.LeakyReLU(),\n",
        "      keras.layers.Dense(1)\n",
        "  ])\n",
        "  \n",
        "  t_actual = model_t(t_actual)\n",
        "  t_random = model_t(t_random)\n",
        "  return t_actual, t_random\n",
        "  # We assume (naively) that action will cause a diff from state -> next state.\n",
        "  # this may not be true for more complicated transition functions.\n",
        "  \n",
        "  # Or ... we don't have to be that complicated... we just need T(s,a,s') -> R\n",
        "  # So why don't we do f(s,a) -> s_pred and then dot(s_pred, s') -> R\n",
        "  \n",
        "  # the network to do f(s,a) -> s_pred is to first convert \n",
        "  # (3, m), (2, n) into (3, 2, m+n), then do fc to (3, 2, m)\n",
        "  # finally add the dim 2 together (assuming that each action dim contributes)\n",
        "  # independently to the next state to get (3, m) again, which is the next state\n",
        "  \n",
        "  # I adopted an RL-agonistic model...\n",
        "\n",
        "\n",
        "def get_v_network(t_actual, t_random):\n",
        "  v = keras.backend.mean(t_actual) - keras.backend.log(keras.backend.mean(keras.backend.exp(t_random)))\n",
        "  return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT68Y46-lLM8",
        "colab_type": "code",
        "outputId": "652bc824-211a-4a0e-b617-e24362855a2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "correlations = np.linspace(-0.9,0.9,15)\n",
        "corr_score = []\n",
        "\n",
        "\n",
        "for correlation in correlations:\n",
        "\n",
        "  state_space = (3,1)\n",
        "  action_space = (2,1)\n",
        "\n",
        "  train_data_size = 10000\n",
        "  test_data_size = train_data_size // 10\n",
        "\n",
        "  env = Env()\n",
        "  x_train = env.get_data(train_data_size, correlation=correlation)\n",
        "  y_train = np.zeros(train_data_size)\n",
        "  x_test = env.get_data(test_data_size, correlation=correlation)\n",
        "  y_test = np.zeros(test_data_size)\n",
        "  x_tiny = env.get_data(1, correlation=correlation)\n",
        "  y_tiny = np.zeros(1)\n",
        "\n",
        "#   x_input = keras.layers.Input(shape=x_shape, dtype='float', name='x_input')\n",
        "#   actual_y_input = keras.layers.Input(shape=y_shape, dtype='float', name='actual_y_input')\n",
        "#   random_y_input = keras.layers.Input(shape=y_shape, dtype='float', name='random_y_input')\n",
        "#   t_actual, t_random = get_t_network_general(x_input, actual_y_input, random_y_input)\n",
        "\n",
        "#   v = get_v_network(t_actual, t_random)\n",
        "\n",
        "\n",
        "  state = keras.layers.Input(shape=state_shape, dtype='float', name='state')\n",
        "  actual_action = keras.layers.Input(shape=action_shape, dtype='float', name='actual_action')\n",
        "  random_action = keras.layers.Input(shape=action_shape, dtype='float', name='random_action')\n",
        "  next_state = keras.layers.Input(shape=state_shape, dtype='float', name='next_state')\n",
        "\n",
        "  t_actual, t_random = get_t_network(state, actual_action, next_state, random_action)\n",
        "\n",
        "  v = get_v_network(t_actual, t_random)\n",
        "\n",
        "\n",
        "  # TODO: the gradient of v is biased. I am not sure how to use the moving average\n",
        "  # yet exactly.\n",
        "\n",
        "  model = keras.models.Model(inputs=[state, actual_action, random_action, next_state], outputs=[v])\n",
        "  optimizer = keras.optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None)\n",
        "\n",
        "\n",
        "  def neg_identity_loss(y_true, y_pred):\n",
        "    # Maximize.\n",
        "    return -y_pred\n",
        "\n",
        "  model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=neg_identity_loss,\n",
        "  #   metrics=[tf.keras.metrics.Mean()]\n",
        "  )\n",
        "\n",
        "  model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "  score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "  print('test score: ', score)\n",
        "  \n",
        "  corr_score.append(score)\n",
        "  break\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples\n",
            "Epoch 1/10\n",
            "10000/10000 [==============================] - 1s 109us/sample - loss: 0.0017\n",
            "Epoch 2/10\n",
            "10000/10000 [==============================] - 1s 93us/sample - loss: 5.5707e-04\n",
            "Epoch 3/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: 4.3522e-04\n",
            "Epoch 4/10\n",
            "10000/10000 [==============================] - 1s 80us/sample - loss: -1.8036e-04\n",
            "Epoch 5/10\n",
            "10000/10000 [==============================] - 1s 84us/sample - loss: 1.4469e-05\n",
            "Epoch 6/10\n",
            "10000/10000 [==============================] - 1s 86us/sample - loss: -4.3617e-05\n",
            "Epoch 7/10\n",
            "10000/10000 [==============================] - 1s 79us/sample - loss: -5.5834e-05\n",
            "Epoch 8/10\n",
            "10000/10000 [==============================] - 1s 77us/sample - loss: -3.2271e-05\n",
            "Epoch 9/10\n",
            "10000/10000 [==============================] - 1s 83us/sample - loss: -6.2067e-05\n",
            "Epoch 10/10\n",
            "10000/10000 [==============================] - 1s 87us/sample - loss: -9.7897e-05\n",
            "1000/1000 [==============================] - 0s 100us/sample - loss: 8.7205e-04\n",
            "test score:  0.0008720472157001495\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}